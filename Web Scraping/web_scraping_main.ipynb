{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: 'file.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m soup \u001b[39m=\u001b[39m bs(page\u001b[39m.\u001b[39mcontent, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[39m# create a file in the same folder as this program and start writing.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mfile.html\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     19\u001b[0m     start \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39mfind_all(line_types):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/web_scraping/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'file.html'"
     ]
    }
   ],
   "source": [
    "# request site text, search for certain tags and write them, then put them in doc. To run again, need to delete file.html and docx file\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pypandoc\n",
    "import pandas as pd\n",
    "\n",
    "#line types in html file that will be checked.\n",
    "line_types = [\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"p\",\"ol\",\"ul\"]  # there is an issue getting the <li> out of nested lists without repeting.\n",
    "\n",
    "# URl to get the html file from\n",
    "URL = \"https://houseofyumm.com/chile-colorado/\"\n",
    "\n",
    "# get webpage and make a html.parser of that page using BeautifulSoup\n",
    "page = requests.get(URL)\n",
    "soup = bs(page.content, \"html.parser\")\n",
    "\n",
    "# create a file in the same folder as this program and start writing.\n",
    "with open('file.html', 'w') as file:\n",
    "    start = True\n",
    "    for data in soup.find_all(line_types):\n",
    "        file.write(str(data))\n",
    "\n",
    "# convert file.html inot a word docx\n",
    "output = pypandoc.convert_file('file.html', 'docx', outputfile=\"file1.docx\")\n",
    "assert output == \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to install these packeages using pip or conda in console before running.\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pypandoc\n",
    "import pandas as pd\n",
    "\n",
    "# This is a list of the line types in html file that will be checked.\n",
    "# TODO There is an issue where ul and ol elements contain li elements.\n",
    "# Also, li elements could contain sublists. Both of these issues\n",
    "# causes problems with writing an elements multiple times in the html file.\n",
    "# Examle: We could write an ul element, and a li lement contained in it. Or a li element\n",
    "# could contain a ul element with further li elements.\n",
    "# One way to solve this would be to write a program that takes an ol or ul and\n",
    "# writes it, does not write all ol and ul contained in it. Then simple do\n",
    "# not search for li elements, since they are contained in ol and ul (right?)\n",
    "\n",
    "line_types = [\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"p\",\"ol\",\"ul\",\"li\"]\n",
    "\n",
    "# we read an excel sheet in the same folder as this file and go down the first column, trying to upen URLs there\n",
    "df = pd.read_excel('Cherri Shelton.xlsx')\n",
    "\n",
    "# We will write a html file holding all the content we scrape from webpages, then save that file to a word doc\n",
    "with open('file.html', 'w') as file:\n",
    "\n",
    "    # we will do the following for each url in the excel column\n",
    "    for URL in df.iloc[:,0]:\n",
    "\n",
    "        # try to open the url. Some entries in the doc may not be URLs, or the links may be broken.\n",
    "        # TODO: Perhaps we should include a case if a URL is broken, as opposed to just passing through.\n",
    "        try:\n",
    "            # try to open the URL and get the content ready to parse using BeautifulSoup\n",
    "\n",
    "            page = requests.get(URL)\n",
    "            soup = bs(page.content, \"html.parser\")\n",
    "            \n",
    "            # create a heading in HTML that will be easily seen when we convert to a word doc\n",
    "            file.write(\"<p>#############################################</p>\")\n",
    "            file.write(URL)\n",
    "            file.write(\"<p>#############################################</p>\")\n",
    "\n",
    "            # the below gets any html element that has a label in \"line_types\" and writes them to file.html.\n",
    "            # It gets the elemnts in the order they appear in the original html file. There is also an attempt\n",
    "            # to avoid unwanted information by checking for a condition until it starts writing content.\n",
    "            # The condition looks for a series of headings (<h1>, <h2>, etc.) followed by a <p> element.\n",
    "            # when this is satisfied, it writed the series of headings, the <p> element, and all later elements.\n",
    "            # TODO this logic is not very good. THe next box has an attempt as better logic.\n",
    "\n",
    "            # start = 0 at the beginning\n",
    "            mode = \"searching for heading\"\n",
    "            for data in soup.find_all(line_types):\n",
    "\n",
    "                # if moe == \"searching for heading\", parser looks for a heading. When a heading is found, it starts storing lines\n",
    "                if mode == \"searching for heading\":\n",
    "                    if data.name == \"h1\" or data.name == \"h2\" or data.name == \"h3\" or data.name == \"h4\" or data.name == \"h5\":\n",
    "                        string_storage = str(data)\n",
    "                        mode = \"looking for paragraph\"\n",
    "                        \n",
    "                # If mode == \"searching for paragraph\", the parser has found a heading, and is searching for paragraphs (<p>). If it finds another heading,\n",
    "                # it will store that and continue looking for paragraphs. If it finds something that is a paragraph, it will write everything it has\n",
    "                # stored and turn to full record mode. If it finds soemthing else, it will turn to \"searching for heading\" mode.\n",
    "                elif mode == \"looking for paragraph\":\n",
    "                    if data.name == \"h1\" or data.name == \"h2\" or data.name == \"h3\" or data.name == \"h4\" or data.name == \"h5\":\n",
    "                        string_storage += str(data)\n",
    "                    elif data.name == \"p\":\n",
    "                        file.write(string_storage + str(data))\n",
    "                        mode = \"full record\"\n",
    "                    else:\n",
    "                        mode = \"searching for heading\"\n",
    "                else:\n",
    "                    file.write(str(data))\n",
    "\n",
    "            # After the whole file has been parsed, we write a newline to separate webpages in the doc file.\n",
    "            file.write('<p><br></p>')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# after we have written the html file with all the websites, we convert the html file to a docx file.\n",
    "output = pypandoc.convert_file('file.html', 'docx', outputfile=\"file1.docx\")\n",
    "assert output == \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is another file parser with some more complex logic.\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pypandoc\n",
    "import pandas as pd\n",
    "\n",
    "# the function takes a data entry and gives the number of words in that entry\n",
    "def get_entry_length(line):\n",
    "    return len(line.get_text().split())\n",
    "\n",
    "\n",
    "line_types = [\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"p\",\"li\"]\n",
    "# we are looking for paragraph or list elements that are a certain length. This may be an indicator of\n",
    "# significant content that should be recorded.\n",
    "length_threshold = 10\n",
    "df = pd.read_excel('Cherri Shelton.xlsx')\n",
    "with open('file.html', 'w') as file:\n",
    "    for URL in df.iloc[:,0]:\n",
    "        try:\n",
    "            page = requests.get(URL)\n",
    "            soup = bs(page.content, \"html.parser\")\n",
    "            file.write(\"<p>#############################################</p>\")\n",
    "            file.write(URL)\n",
    "            file.write(\"<p>#############################################</p>\")\n",
    "            string_storage = ''\n",
    "            recording = False\n",
    "            recording_body = False\n",
    "            commit = False\n",
    "            for data in soup.find_all(line_types):\n",
    "                # The logic looks for a section of text of a cerain form. It looks for a series of headers, followed by body content line with lots of words.\n",
    "                # If it finds this, it will record it and start searching again.\n",
    "\n",
    "                # The logic checks for a heading first. When it finds a heading, it starts recording lines and checks for more headings, paragraphs, or lists.\n",
    "                # If it finds a paragraph or list element with a length of text that is long (>=10), it will write the saved chunk of headings and paragraphs,\n",
    "                # and continue to write paragraphs and list elelements until if finds another header. When it finds another header, it will reset.\n",
    "\n",
    "                # TODO: The search does not look for ol or ul items, so the bullet and numbered format is absent in the word doc. Also, the \n",
    "                # get_entry_length function has issues.\n",
    "\n",
    "                # TODO There is also the issue outlined in cell 2 with searching for just ol and ul items.\n",
    "                \n",
    "                # TODO If given a ul/ol item, get_entry_length adds up all text in all contained li elements and returns the sum.\n",
    "                # It sould return 0. If given a li element containing a ul or ol, get_entry_length will add up all the\n",
    "                # words in the ul/ol item.\n",
    "                if not recording:\n",
    "                    if data.name == \"h1\" or data.name == \"h2\" or data.name == \"h3\" or data.name == \"h4\" or data.name == \"h5\":\n",
    "                        string_storage = str(data)\n",
    "                        recording = True\n",
    "                elif not recording_body:\n",
    "                    if data.name == \"h1\" or data.name == \"h2\" or data.name == \"h3\" or data.name == \"h4\" or data.name == \"h5\":\n",
    "                        string_storage += str(data)\n",
    "                    elif data.name == \"p\" or data.name == \"li\":\n",
    "                        string_storage += str(data)\n",
    "                        recording_body = True\n",
    "                        if get_entry_length(data)>=length_threshold:\n",
    "                            commit = True\n",
    "                elif not commit:\n",
    "                    if data.name == \"h1\" or data.name == \"h2\" or data.name == \"h3\" or data.name == \"h4\" or data.name == \"h5\":\n",
    "                        string_storage = str(data)\n",
    "                        recording_body = False\n",
    "                    else:\n",
    "                        string_storage += str(data)\n",
    "                        if get_entry_length(data)>=length_threshold:\n",
    "                            commit = True\n",
    "                else:\n",
    "                    # Here is where we write to file. We have recorded a heading followed by body content of a certain length in string_storage,\n",
    "                    # and have come to another heading. We write the chunk we recorded and begin looking for a body of a certain length.\n",
    "                    if data.name == \"h1\" or data.name == \"h2\" or data.name == \"h3\" or data.name == \"h4\" or data.name == \"h5\":\n",
    "                        file.write(string_storage)\n",
    "                        string_storage = str(data)\n",
    "                        recording_body = False\n",
    "                        commit = False\n",
    "                    else:\n",
    "                        string_storage += str(data)\n",
    "            \n",
    "            # If we ended the file and want to commit waht is recorded, we do it here.\n",
    "            if commit:\n",
    "                file.write(string_storage)\n",
    "            file.write('<p><br></p>')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "output = pypandoc.convert_file('file.html', 'docx', outputfile=\"file1.docx\")\n",
    "assert output == \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9444efb63d0d06add65d588e3a1c3da4d5064b056c3315c270e96dfed4e4b1db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
